{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Niral Shah\n",
    "## ML HW 3\n",
    "### 02/16/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition $\\textit{convex_hull }:\n",
    "$\n",
    "\n",
    "$x=\\sum_{n} \\alpha_nx_n$ , where $\\alpha_n \\geq 0 $ and $\\sum_n \\alpha_n = 1$\n",
    "\n",
    "Consider a second set of points {$x'_m$} with a corresponding convex hull. Then the two sets of points will be linearly separable if there exists a vector $\\textbf{w}$ and a scalar $w_0$ such that:\n",
    "\n",
    "$w^{T}x_n + w_0 > 0$, for all  $x_n$ and $ w^{T}x_n'+ w_0 < 0$\n",
    "\n",
    "**Prove that if two sets of points are linearly seperable, their convex hulls do not intersect:**\n",
    "\n",
    "For the set of points in ${\\{x^n\\}}$ it will correspond to:\n",
    "\n",
    "* $w^{T}(\\sum_{n} \\alpha_nx_n) + w_0 > 0$ \n",
    "\n",
    "Using the definition of the convex hull (Since $\\sum_n \\alpha_n = 1$) \n",
    ", we can simplify this to:\n",
    "\n",
    "* $\\sum_{n}\\alpha_n(w^{T}x_n + w_0) > 0$\n",
    "\n",
    "Similarly, for the set of ponts in $\\{x_n'\\}$ it will correspond to: \n",
    "* $\\sum_{n}\\alpha_n(w^{T}x_n'+ w_0) < 0$\n",
    "\n",
    "** Proof:** \n",
    "* Let's assume that the convex hulls intersect. \n",
    "\n",
    "This means there exists a point, $x_{nm}$ $\\in$ $\\{x_n\\}$ & $\\{x_n'\\}$. Which implies that at $x_{nm}$ : \n",
    "\n",
    "$\\sum_{n} \\alpha_n(w^{T}x_{nm} + w_0) = \\sum_{n} \\alpha_n(w^{T}x_{nm} + w_0) $ \n",
    "\n",
    "However this is impossible to simultaneously satisfy if they are linearly separable, since:\n",
    "$\\sum_{n} \\alpha_n(w^{T}x_{nm} + w_0)> 0$ & $\\sum_{n} \\alpha_n(w^{T}x_{nm} + w_0) < 0 $  \n",
    "\n",
    "Thus this implies that if the convex hulls intersect then their sets are not linearly separable. Which by the **contrapositive proves that if two sets of points are linearly separble then their convex hulls do not intersect**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Logistic regression and gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2a: \n",
    "\n",
    "$\\sigma{'(a)}$ = $\\frac{\\partial}{\\partial{a}}(1+e^{-a})^{-1}$\n",
    "\n",
    "$= -(1+e^{-a})^{-2} (-e^{-a})$\n",
    "\n",
    "$= -(1+2e^{-a} + e^{-2a})^{-1} (-e^{-a})$\n",
    "\n",
    "$=\\frac{e^{-a}}{(1+2e^{-a} + e^{-2a})} $\n",
    "\n",
    "Thus:\n",
    "$\\sigma{'(a)} = \\frac{e^{-a}}{(1+2e^{-a} + e^{-2a})} $ \n",
    "\n",
    "**Now does this mean:**\n",
    "\n",
    "$\\sigma{(a)}(1-\\sigma{(a)}) \\stackrel{?}{=} \\sigma{'(a)}$\n",
    "\n",
    "$=(\\frac{1}{(1+e^{-a})})(1-\\frac{1}{(1+e^{-a})})$\n",
    "\n",
    "$=\\frac{(1+e^{-a})-1}{(1+2e^{-a} + e^{-2a})} $\n",
    "\n",
    "$=\\frac{e^{-a}}{(1+2e^{-a} + e^{-2a})} $ \n",
    "\n",
    "This proves that **yes**,\n",
    "\n",
    "$\\sigma{(a)}(1-\\sigma{(a)}) \\stackrel{\\checkmark}{=} \\sigma{'(a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2b: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute first derivative: \n",
    "\n",
    "$\\frac{\\partial{L\\{x^{(i)},y^{(i)}\\}\\stackrel{n}{i=1} }}{\\partial{w_j}}$\n",
    "\n",
    "$=\\frac{\\partial}{\\partial{w}}\\sum_{i=1}^{n} \\frac{-y_i}{\\sigma{(w^Tx)}} x\\sigma{(w^Tx)}(1-\\sigma{(w^Tx)}) - \n",
    "\\frac{(1-y_i)}{1-\\sigma{(w^Tx)}} (-x)\\sigma{(w^Tx)}(1-\\sigma{(w^Tx)})\n",
    "$\n",
    "\n",
    "$=\\sum_{i=1}^{n} -x_iy_i(1-\\sigma{(w^Tx)}) + x_i(1-y_i)\\sigma{(w^Tx_i)}$\n",
    "\n",
    "$=\\sum_{i=1}^{n} -x_iy_i + x_i\\sigma{(w^Tx_i)}$\n",
    "\n",
    "Thus: \n",
    "$\\frac{\\partial}{\\partial{w}} = \\sum_{i=1}^{n} -x_iy_i + x_i\\sigma{(w^Tx_i)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem 2c: \n",
    "\n",
    "To prove convexity, the second derivative \n",
    "$\\frac{\\partial^2{L}}{\\partial{w^2}} \\geq 0$ (The Hessian Matrix is PSD)\n",
    "\n",
    "Thus computing the second derivative:\n",
    "\n",
    "$\\frac{\\partial^2{L}}{\\partial{w^2}}$\n",
    "$=\\frac{\\partial}{\\partial{w}} \\sum_{i=1}^{n} -x_iy_i + x_i\\sigma{(w^Tx_i)} $\n",
    "\n",
    "$=\\sum_{i=1}^{n} x_i^{2}\\sigma{(w^Tx_i)}(1-\\sigma{(w^Tx_i)})$\n",
    "\n",
    "Since $\\sigma$ by definition is between [0,1] and $x_i^2 \\geq 0 $. The second derivative $\\frac{\\partial^2{L}}{\\partial{w^2}} \\geq 0 $ This also implies the Hessian matrix (diagonal) is postive semi definite. \n",
    "\n",
    "#### Problem 2d: (see attached notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3a: \n",
    "\n",
    "From the classnotes, it is proven that the Training Error Decays exponentially fast if the weak learning assumption holds. \n",
    "This is represented by the fact that the misclassification error is upper bounded by the exponential loss: \n",
    "\n",
    "$\\frac{1}{n}{\\sum_{i=1}^{n}{1_[y_i \\neq H(x_i)]}} \\leq e^{-2\\gamma_{WLA}^2T}$\n",
    "\n",
    "Taking the limit of T, the number of iterations, goes to infinity:\n",
    "\n",
    "$\\lim_{T \\to \\infty}{e^{-2\\gamma_{WLA}^2T}} = 0 $ \n",
    "\n",
    "Since this represents the upper bound, this means that the boosted model eventually classifies the training set perfectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3b: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^{train}(\\lambda) = \\sum_i{w_ie^{-(M\\lambda)}}$\n",
    "\n",
    "$j_t \\in argmax[\\frac{-\\partial(R^{(train)}}{\\partial{\\alpha}}(\\lambda + \\alpha e_j)]_{\\alpha=0}$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$j_t \\in argmax[\\frac{-\\partial}{\\partial{\\alpha}}[\\sum_{i=1}^{n}w_ie^{-(M(\\lambda+\\alpha e_j))_i}]]_{\\alpha=0}$\n",
    "\n",
    "$\\vdots$ \n",
    "\n",
    "$j_t \\in argmax[\\sum_{i=1}^{n}{w_iM_{ij}e^{-(M\\lambda_t)_i}}] $\n",
    "\n",
    "Since $w_i$ is not dependent on $j_t$ we can remove it from the argmax term. \n",
    "\n",
    "$\\vdots$ \n",
    "\n",
    "This implies that: \n",
    "\n",
    "$d_{ti} = \\frac{w_ie^{(-M\\lambda_t)_i}}{Z_t} $\n",
    "\n",
    "$Z_t = \\sum_{i=1}^{n} w_ie^{(-M\\lambda_t)_i} $ \n",
    "\n",
    "$\\alpha_t = 0.5*\\ln{\\frac{1-d_-}{d_-}} $ (no change for alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3c: (see attached notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
